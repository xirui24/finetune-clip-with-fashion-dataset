{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune CLIP for fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import model as fashion_clip\n",
    "\n",
    "# training hyperparameters\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 8\n",
    "LR = 1e-6\n",
    "WD = 1e-4\n",
    "patience = 2\n",
    "\n",
    "# LoRA hyperparameters\n",
    "train_with_lora = True\n",
    "r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "bias = \"all\"\n",
    "\n",
    "target_modules = [\"visual_projection\",\"text_projection\"]\n",
    "\"\"\"\"options for target_modules:\n",
    "    [\"k_proj\", \"q_proj\", \"v_proj\", \"out_proj\"]: QKV attention and the output of transformer's attention modules\n",
    "    [\"k_proj\", \"q_proj\", \"v_proj\"]: only QKV attention\n",
    "    [\"fc1\",\"fc2\"]: transformer's MLP modules\n",
    "    [\"visual_projection\",\"text_projection\"]: projection of visual and text embeds calculated by encoders to the commun space (equivalent to freeze the encoders)\"\"\"\n",
    "\n",
    "\n",
    "# data\n",
    "adjust_data_size = False\n",
    "save_dir = \"./results/\"\n",
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample smaller datasets for quick test, with a balanced distribution for each class label\n",
    "if adjust_data_size:\n",
    "    fashion_clip.adjust_dataset_size(\"./data/train_data.json\", \"./data/small_train.json\", 200)\n",
    "    fashion_clip.adjust_dataset_size(\"./data/val_data.json\", \"./data/small_val.json\", 40)\n",
    "    fashion_clip.adjust_dataset_size(\"./data/test_data.json\", \"./data/small_test.json\", 40)\n",
    "\n",
    "# load data\n",
    "train_data = fashion_clip.load_data(\"./data/small_train.json\")\n",
    "val_data = fashion_clip.load_data(\"./data/small_val.json\")\n",
    "test_data = fashion_clip.load_data(\"./data/small_test.json\")\n",
    "\n",
    "labels = list(set([data[\"class_label\"] for data in train_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an example\n",
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "if train_with_lora:\n",
    "    print(\"Number of trainable parameters without LoRA:\")\n",
    "    fashion_clip.print_trainable_parameters(model)\n",
    "    model = fashion_clip.load_lora_model(model, DEVICE, target_modules, r, lora_alpha, lora_dropout, bias)\n",
    "    print()\n",
    "    print(\"Number of trainable parameters with LoRA:\")\n",
    "    fashion_clip.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "train_dataloader = fashion_clip.DataGenerator(train_data, data_dir)\n",
    "val_dataloader = fashion_clip.DataGenerator(val_data, data_dir)\n",
    "\n",
    "if not train_with_lora:\n",
    "    # data to be sent to the trainer\n",
    "    train_dataset = list(train_dataloader.get_dataset(processor, DEVICE))\n",
    "    val_dataset = list(val_dataloader.get_dataset(processor, DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation before training as a baseline\n",
    "# evaluation task: prediction on class label\n",
    "\n",
    "texts = []\n",
    "for cl in labels:\n",
    "    texts.append(f\"a photo of{cl}\")\n",
    "\n",
    "\n",
    "# top3 prediction for a single image\n",
    "index = random.randint(0, len(test_data))           # randomly choose an example of the testset\n",
    "image = Image.open(data_dir+test_data[index]['image_path'])\n",
    "gold_label = test_data[index][\"class_label\"]        # gold label of the chosen example\n",
    "\n",
    "text_features, image_feature = fashion_clip.get_features(texts, image, model, processor, DEVICE)    # get text and image features\n",
    "fashion_clip.make_single_prediction(text_features, image_feature, 3, labels)\n",
    "print(f\"Correct label: {gold_label}\")\n",
    "\n",
    "\n",
    "# top1 precision for all images in test data\n",
    "images = [Image.open(data_dir+data['image_path']) for data in test_data]        # preprocess all images in the testset\n",
    "all_gold_labels = [labels.index(data[\"class_label\"]) for data in test_data]     # gold labels of all images in the testset\n",
    "\n",
    "image_features = fashion_clip.image_features(images, model, processor, DEVICE)  # we're using the same texts so no need to recalculate text features here\n",
    "fashion_clip.make_full_prediction(text_features, image_features, all_gold_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_with_lora:\n",
    "    # use Huggingface's Trainer to finetune the model, the best model will be saved in save_dir\n",
    "    clip_trainer = fashion_clip.FashionCLIPTrainer(model, train_dataset, val_dataset, save_dir, LR, WD, patience, BATCH_SIZE, EPOCH)\n",
    "    clip_trainer.trainer.train()\n",
    "\n",
    "    clip_trainer.trainer.evaluate()\n",
    "\n",
    "    # plot train and val loss\n",
    "    log_history = clip_trainer.trainer.state.log_history\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    for log in log_history[:-1]:\n",
    "        if \"eval_loss\" in log:\n",
    "            eval_losses.append(log[\"eval_loss\"])\n",
    "        if \"loss\" in log:\n",
    "            train_losses.append(log[\"loss\"])\n",
    "\n",
    "    plt.plot(train_losses, label=\"train loss\")\n",
    "    plt.plot(eval_losses, label=\"val loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(save_dir+\"loss.png\")\n",
    "\n",
    "\n",
    "else: \n",
    "    # train with LoRA\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_data)*EPOCH)\n",
    "    save_model = fashion_clip.SaveModel(dir=save_dir)\n",
    "    logger = fashion_clip.Logger(save_dir, BATCH_SIZE, LR, WD, r, lora_alpha, lora_dropout, bias, target_modules)\n",
    "\n",
    "    # train and validate\n",
    "    fashion_clip.train_and_validate(model, processor, train_dataloader, val_dataloader, optimizer, scheduler, save_model, EPOCH, BATCH_SIZE, DEVICE, save_dir, logger, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 3 prediction of a single image\n",
    "text_features, image_feature = fashion_clip.get_features(texts, image, model, processor, DEVICE)\n",
    "fashion_clip.make_single_prediction(text_features, image_feature,3,labels)\n",
    "print(f\"Correct label: {gold_label}\")\n",
    "\n",
    "# top 1 accuracy of all images\n",
    "image_features = fashion_clip.image_features(images, model, processor, DEVICE)\n",
    "fashion_clip.make_full_prediction(text_features, image_features, all_gold_labels, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
