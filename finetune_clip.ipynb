{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune CLIP for fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import model as fashion_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 4\n",
    "LR = 1e-6\n",
    "WD = 1e-4\n",
    "patience = 2\n",
    "\n",
    "adjust_data_size = False\n",
    "\n",
    "save_dir = \"./results/\"\n",
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample smaller datasets for quick test, with a balanced distribution for each class label\n",
    "if adjust_data_size:\n",
    "    fashion_clip.adjust_dataset_size(\"./data/train_data.json\", \"./data/small_train.json\", 200)\n",
    "    fashion_clip.adjust_dataset_size(\"./data/val_data.json\", \"./data/small_val.json\", 40)\n",
    "    fashion_clip.adjust_dataset_size(\"./data/test_data.json\", \"./data/small_test.json\", 40)\n",
    "\n",
    "# load data\n",
    "train_data = fashion_clip.load_data(\"./data/train_data.json\")\n",
    "val_data = fashion_clip.load_data(\"./data/val_data.json\")\n",
    "test_data = fashion_clip.load_data(\"./data/test_data.json\")\n",
    "\n",
    "labels = list(set([data[\"class_label\"] for data in train_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# create dataset\n",
    "train_dataset = list(fashion_clip.get_dataset(train_data, data_dir, processor, DEVICE))\n",
    "val_dataset = list(fashion_clip.get_dataset(val_data, data_dir, processor, DEVICE))\n",
    "test_dataset = list(fashion_clip.get_dataset(test_data, data_dir, processor, DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation before training as a baseline\n",
    "# evaluation task: prediction on class label\n",
    "\n",
    "texts = []\n",
    "for cl in labels:\n",
    "    texts.append(f\"a photo of{cl}\")\n",
    "\n",
    "# top3 prediction for a single image\n",
    "index = random.randint(0, len(test_data))           # randomly choose an example of the testset\n",
    "image = Image.open(data_dir+test_data[index]['image_path'])\n",
    "gold_label = test_data[index][\"class_label\"]        # gold label of the chosen example\n",
    "\n",
    "text_features, image_feature = fashion_clip.get_features(texts, image, model, processor, DEVICE)    # get text and image features\n",
    "fashion_clip.make_single_prediction(text_features, image_feature, 3, labels)\n",
    "print(f\"Correct label: {gold_label}\")\n",
    "\n",
    "# top1 precision for all images in test data\n",
    "images = [Image.open(data_dir+data['image_path']) for data in test_data]        # preprocess all images in the testset\n",
    "all_gold_labels = [labels.index(data[\"class_label\"]) for data in test_data]     # gold labels of all images in the testset\n",
    "\n",
    "image_features = fashion_clip.image_features(images, model, processor, DEVICE)  # we're using the same texts so no need to recalculate text features here\n",
    "fashion_clip.make_full_prediction(text_features, image_features, all_gold_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Huggingface's Trainer to finetune the model, the best model will be saved in save_dir\n",
    "clip_trainer = fashion_clip.FashionCLIPTrainer(model, train_dataset, val_dataset, save_dir, LR, WD, patience, BATCH_SIZE, EPOCH)\n",
    "clip_trainer.trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_trainer.trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and val loss\n",
    "log_history = clip_trainer.trainer.state.log_history\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "for log in log_history[:-1]:\n",
    "    if \"eval_loss\" in log:\n",
    "        eval_losses.append(log[\"eval_loss\"])\n",
    "    if \"loss\" in log:\n",
    "        train_losses.append(log[\"loss\"])\n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(eval_losses, label=\"val loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.savefig(save_dir+\"loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 3 prediction of a single image\n",
    "text_features, image_feature = fashion_clip.get_features(texts, image, model, processor, DEVICE)\n",
    "fashion_clip.make_single_prediction(text_features, image_feature,3,labels)\n",
    "print(f\"Correct label: {gold_label}\")\n",
    "\n",
    "# top 1 accuracy of all images\n",
    "image_features = fashion_clip.image_features(images, model, processor, DEVICE)\n",
    "fashion_clip.make_full_prediction(text_features, image_features, all_gold_labels, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
